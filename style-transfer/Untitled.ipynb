{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer with Deep Neural Networks\n",
    "In this notebook, weâ€™ll recreate a style transfer method that is outlined in the paper, Image Style Transfer Using Convolutional Neural Networks, by Gatys in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the vgg 19 network from the Py-torch library\n",
    "\n",
    "###### vgg19.features  are convolutional  and pooling layers\n",
    "###### vgg19.classifier  are convolutional  and pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained = True).features\n",
    "\n",
    "\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "vgg.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Content and Style Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path , max_size = 400 , shape= None):\n",
    "    \n",
    "    if \"https\" in img_path:\n",
    "        \n",
    "        respose = requests.get(img_path)\n",
    "        image   = Image.open()\n",
    "        \n",
    "    else:\n",
    "        image   = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "    ## decrease the size of the image\n",
    "    if max(image.size) > max_size:\n",
    "                    size = max_size \n",
    "    else :\n",
    "        size = max(image.size)\n",
    "        \n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    transformer = transforms.Compose([ transforms.Resize(size),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                                           (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    image = transformer(image)[:3 , : , :].unsqueeze(0)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the content  image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = load_image('images/octopus.jpg').to(device)\n",
    "\n",
    "style   = load_image('images/hockney.jpg' , shape=content.shape[-2:]).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# un normalizing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im_convert(tensor):\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    \n",
    "    image = image.numpy().squeeze()\n",
    "    \n",
    "    image = image.transpose(1 , 2, 0)\n",
    "    \n",
    "    image = image*np.array((0.229, 0.224, 0.225))+ np.array((0.485, 0.456, 0.406))\n",
    "        \n",
    "    image = image.clip(0,1)\n",
    "    \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "# content and style ims side-by-side\n",
    "ax1.imshow(im_convert(content))\n",
    "ax2.imshow(im_convert(style))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content and Style Features\n",
    "Below, complete the mapping of layer names to the names found in the paper for the content representation and the style representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model, layers = None):\n",
    "    \n",
    "    if layers is None:\n",
    "        layers = { '0' : 'conv1_1',\n",
    "                   '5' : 'conv2_1',\n",
    "                   '10': 'conv3_1',\n",
    "                   '19': 'conv4_1',\n",
    "                   '21': 'conv4_2',\n",
    "                   '28': 'conv5_1',}\n",
    "        \n",
    "    features ={}\n",
    "    x = image\n",
    "    \n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  calculate the Gram Matrix\n",
    "\n",
    "Get the depth, height, and width of a tensor using batch_size, d, h, w = tensor.size\n",
    "Reshape that tensor so that the spatial dimensions are flattened\n",
    "Calculate the gram matrix by multiplying the reshaped tensor by it's transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \n",
    "    _, depth , height, width = tensor.size()\n",
    "    \n",
    "    tensor = tensor.view(depth, height*width)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #calculating  the gram matrix\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    \n",
    "    \n",
    "    return gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_features  = get_features(content, vgg)\n",
    "style_features    = get_features(style, vgg) \n",
    "\n",
    "\n",
    "\n",
    "# Loss calculation:\n",
    "style_grams = {layer : gram_matrix(style_features[layer]) for layer in style_features }\n",
    "target = content.clone().requires_grad_(True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Weights\n",
    "\n",
    "\n",
    "Content and Style Weight\n",
    "Just like in the paper, we define an alpha (content_weight) and a beta (style_weight). This ratio will affect how stylized your final image is. It's recommended that you leave the content_weight = 1 and set the style_weight to achieve the ratio you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_weights  ={'conv1_1' : 1.,\n",
    "                 'conv2_1'  : 0.75,\n",
    "                 'conv2_1'  : 0.2,\n",
    "                 'conv2_1'  : 0.2,\n",
    "                 'conv2_1'  : 0.2}\n",
    "\n",
    "\n",
    "content_weight = 1  # alpha\n",
    "style_weight = 1e6  # beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content Loss\n",
    "The content loss will be the mean squared difference between the target and content features at layer conv4_2. This can be calculated as follows:\n",
    "\n",
    "content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\n",
    "Style Loss\n",
    "The style loss is calculated in a similar way, only you have to iterate through a number of layers, specified by name in our dictionary style_weights.\n",
    "\n",
    "You'll calculate the gram matrix for the target image, target_gram and style image style_gram at each of these layers and compare those gram matrices, calculating the layer_style_loss. Later, you'll see that this value is normalized by the size of the layer.\n",
    "\n",
    "Total Loss\n",
    "Finally, you'll create the total loss by adding up the style and content losses and weighting them with your specified alpha and beta!\n",
    "\n",
    "Intermittently, we'll print out this loss; don't be alarmed if the loss is very large. It takes some time for an image's style to change and you should focus on the appearance of your target image rather than any loss value. Still, you should see that this loss decreases over some number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_every = 400\n",
    "\n",
    "optimizer = optim.Adam([target], lr = 0.003)\n",
    "\n",
    "steps     = 2000\n",
    "for ii in range(1 , steps +1):\n",
    "    \n",
    "    \n",
    "    target_features = get_features(target, vgg)\n",
    "    \n",
    "    \n",
    "    content_loss    = torch.mean((target_features['conv4_2'] -\n",
    "                                  content_features['conv4_2'])**2)\n",
    "    style_loss = 0 \n",
    "    \n",
    "    \n",
    "    for layer in style_weights:\n",
    "        target_feature = target_features[layer]\n",
    "        target_gram = gram_matrix(target_feature)\n",
    "        _, d, h, w = target_feature.shape\n",
    "        style_gram = style_grams[layer]\n",
    "        # the style loss for one layer, weighted appropriately\n",
    "        layer_style_loss = style_weights[layer] * \\\n",
    "                            torch.mean((target_gram - style_gram)**2)\n",
    "        # add to the style loss\n",
    "        style_loss += layer_style_loss / (d * h * w)\n",
    "        \n",
    "        #total loss \n",
    "    total_loss   = content_weight * \\\n",
    "                   content_loss +   \\\n",
    "                   style_weight *   \\\n",
    "                   style_loss\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    if ii % show_every ==0:\n",
    "        print('total loss ' , total_loss.items())\n",
    "        plt.imshow(in_convert(target))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
