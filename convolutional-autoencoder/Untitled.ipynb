{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder\n",
    "Sticking with the MNIST dataset, let's improve our autoencoder's performance using convolutional layers. We'll build a convolutional autoencoder to compress the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2698857.65it/s]                                                                                                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 88690.09it/s]                                                                                                                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1308771.12it/s]                                                                                                                                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 31025.46it/s]                                                                                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "\n",
    "\n",
    "train_data = datasets.MNIST(root ='data', train= True,\n",
    "                           download =True, transform = transform)\n",
    "\n",
    "test_data  = datasets.MNIST(root='data', train = False,\n",
    "                           download= True, transform= transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 0\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, num_workers= num_workers)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size , num_workers= num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c7aef12e48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c7aed7a8d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "#\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize =(5, 5))\n",
    "ax  = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap ='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "The encoder part of the network will be a typical convolutional pyramid. Each convolutional layer will be followed by a max-pooling layer to reduce the dimensions of the layers.\n",
    "\n",
    "## Decoder\n",
    "The decoder though might be something new to you. The decoder needs to convert from a narrow representation to a wide, reconstructed image. For example, the representation could be a 7x7x4 max-pool layer. This is the output of the encoder, but also the input to the decoder. We want to get a 28x28x1 image out from the decoder so we need to work our way back up from the compressed representation. A schematic of the network is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        #encoder\n",
    "        self.conv1 = nn.Conv2d(1 , 16, 3 , padding =1 )\n",
    "        self.conv2 = nn.Conv2d(16 , 4 , 3, padding =1 )\n",
    "        self.pool  = nn.MaxPool2d(2 , 2)\n",
    "        \n",
    "        #decoder\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4 , 16 , 2 , stride = 2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16 , 1 , 2 , stride = 2)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, x ):\n",
    "    \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ConvAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.312064\n",
      "Epoch: 2 \tTraining Loss: 0.267611\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        images, _ = data\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        #print('type of outputs :', type(outputs) )\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
